{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Prompts and Chains"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import time\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.chains import SequentialChain, LLMChain\n",
    "from langchain.output_parsers import StructuredOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Model Version\n",
    "llm_model = 'gpt-3.5-turbo-0125'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Response Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Extraction\n",
    "sentiment_schema = ResponseSchema(name=\"sentiment\",\n",
    "                                description='''Determine the degree of sentiment of the news article with respect to {company}.\n",
    "Output in the range of (-1, 1) where -1 is Negative, and 1 is Positive.''')\n",
    "\n",
    "# Evidence Extraction\n",
    "evidence_schema = ResponseSchema(name=\"evidence\",\n",
    "                                description='''Extract any sentences that provide evidence for the extracted sentiment, and output as a Python List.''')\n",
    "\n",
    "# Stock Movement Extraction\n",
    "stock_movement_schema = ResponseSchema(name=\"stock_movement\",\n",
    "                                description='''Given your expertise in the field, determine stock movement of the {company}.\n",
    "Output as Up or Down.''')\n",
    "\n",
    "# Explination Extraction\n",
    "explaination_schema = ResponseSchema(name=\"explaination\",\n",
    "                                description='''Explain your thoughts and thinking process. Output as string.''')\n",
    "\n",
    "response_schemas = [\n",
    "    sentiment_schema, \n",
    "    evidence_schema,\n",
    "    stock_movement_schema,\n",
    "    explaination_schema\n",
    "]\n",
    "\n",
    "# Output Parsers and Format Instructions for LLM\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st Chain\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "\n",
    "# Company Extraction Prompt\n",
    "first_prompt = ChatPromptTemplate.from_template(\n",
    "    '''\n",
    "    You are an expert in the field of finance news, stock market and trading. Which company does the news article talk about majorly? Output only one company name.\n",
    "    article: {text}\n",
    "    '''\n",
    ")\n",
    "\n",
    "# Chain 1\n",
    "chain_one = LLMChain(llm=llm, prompt=first_prompt, output_key='company')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd Chain\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\\\n",
    "    You are an expert in analyzing financial news, stock market and trading for {company} given below. For the following financial news article, do the following:\n",
    "\n",
    "    sentiment: Determine the degree of sentiment of the news article with respect to {company}.\n",
    "    Output in the range of (-1, 1) where -1 is Negative, and 1 is Positive.\n",
    "\n",
    "    evidence: Extract any sentences that provide evidence for the extracted sentiment, and output as a Python List.\n",
    "\n",
    "    stock_movement: Given your expertise in the field, determine stock movement of the {company}.\n",
    "    Output as Up or Down.\n",
    "\n",
    "    Explaination: Explain your thoughts and thinking process. Output as string.\n",
    "\n",
    "    article: {text}\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "# chain 2\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt, output_key='json_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall_chain: input = Text \n",
    "# and output= company, json_output\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain_one, chain_two],\n",
    "    input_variables=[\"text\", \"format_instructions\"],\n",
    "    output_variables=[\"company\", \"json_output\"],\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with get_openai_callback() as cb:\n",
    "#     outputs = overall_chain.invoke({\n",
    "#         'text': df.iloc[20, -1],\n",
    "#         'format_instructions': format_instructions\n",
    "#     })\n",
    "#     print(cb.successful_requests)\n",
    "#     print(cb.total_cost)\n",
    "#     print(cb.total_tokens)\n",
    "#     print(cb.prompt_tokens)\n",
    "#     print(cb.completion_tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Requests per minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENS_PER_MINUTE = 58000\n",
    "REQUESTS_PER_MINUTE = 250\n",
    "REQUESTA_PER_DAY = 9000\n",
    "\n",
    "REQUEST_INTERVAL = 61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/final/artilces_with_content.csv')\n",
    "\n",
    "word2token = 3.6\n",
    "df['est_tokens'] = df['Content'].apply(lambda x: len(x.split(' '))) * word2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "\n",
    "while len(df)> 0:\n",
    "    cum_sum = df['est_tokens'].cumsum()\n",
    "    num_requests = cum_sum[cum_sum < TOKENS_PER_MINUTE].size\n",
    "\n",
    "    if num_requests * 2 < REQUESTS_PER_MINUTE: # 2 requests per minute\n",
    "        batches.append(df.iloc[:num_requests, :])\n",
    "        df = df.iloc[num_requests:, :]\n",
    "    else:\n",
    "        batches.append(df.iloc[:REQUESTS_PER_MINUTE, :])\n",
    "        df = df.iloc[REQUESTS_PER_MINUTE:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "for batch in batches:\n",
    "    if len(batch) > REQUESTS_PER_MINUTE or batch['est_tokens'].sum() > TOKENS_PER_MINUTE:\n",
    "        print('Something is Wrong')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run All Batch Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23/23 [01:23<00:00,  3.65s/it]\n",
      "100%|██████████| 23/23 [01:17<00:00,  3.37s/it]\n",
      "100%|██████████| 19/19 [00:58<00:00,  3.06s/it]\n",
      "100%|██████████| 20/20 [01:07<00:00,  3.39s/it]\n",
      "100%|██████████| 27/27 [01:27<00:00,  3.23s/it]\n",
      "100%|██████████| 23/23 [01:13<00:00,  3.20s/it]\n",
      "100%|██████████| 25/25 [01:19<00:00,  3.18s/it]\n",
      "100%|██████████| 22/22 [01:11<00:00,  3.23s/it]\n",
      "100%|██████████| 21/21 [01:10<00:00,  3.35s/it]\n",
      "100%|██████████| 9/9 [00:29<00:00,  3.23s/it]\n",
      "100%|██████████| 18/18 [00:57<00:00,  3.18s/it]\n",
      "100%|██████████| 6/6 [00:20<00:00,  3.37s/it]\n",
      "100%|██████████| 17/17 [00:58<00:00,  3.45s/it]\n",
      "100%|██████████| 14/14 [00:43<00:00,  3.13s/it]\n",
      "100%|██████████| 20/20 [01:01<00:00,  3.07s/it]\n",
      "100%|██████████| 14/14 [00:46<00:00,  3.34s/it]\n",
      "100%|██████████| 21/21 [01:03<00:00,  3.03s/it]\n",
      "100%|██████████| 16/16 [00:52<00:00,  3.25s/it]\n",
      "100%|██████████| 25/25 [01:22<00:00,  3.29s/it]\n",
      "100%|██████████| 29/29 [01:28<00:00,  3.04s/it]\n",
      "100%|██████████| 29/29 [01:36<00:00,  3.31s/it]\n",
      "100%|██████████| 19/19 [01:06<00:00,  3.49s/it]\n",
      "100%|██████████| 22/22 [01:07<00:00,  3.07s/it]\n",
      "100%|██████████| 24/24 [01:14<00:00,  3.11s/it]\n",
      "100%|██████████| 24/24 [01:15<00:00,  3.15s/it]\n",
      "100%|██████████| 24/24 [01:18<00:00,  3.26s/it]\n",
      "100%|██████████| 24/24 [01:15<00:00,  3.15s/it]\n",
      "100%|██████████| 28/28 [01:29<00:00,  3.21s/it]\n",
      "100%|██████████| 20/20 [01:04<00:00,  3.21s/it]\n",
      "100%|██████████| 20/20 [01:06<00:00,  3.31s/it]\n",
      "100%|██████████| 19/19 [01:04<00:00,  3.42s/it]\n",
      "100%|██████████| 17/17 [00:51<00:00,  3.05s/it]\n",
      "100%|██████████| 21/21 [01:07<00:00,  3.22s/it]\n",
      "100%|██████████| 24/24 [01:17<00:00,  3.22s/it]\n",
      "100%|██████████| 20/20 [01:03<00:00,  3.15s/it]\n",
      "100%|██████████| 22/22 [01:10<00:00,  3.19s/it]\n",
      "100%|██████████| 17/17 [00:53<00:00,  3.12s/it]\n",
      "100%|██████████| 12/12 [00:40<00:00,  3.34s/it]\n",
      "100%|██████████| 21/21 [01:12<00:00,  3.46s/it]\n",
      "100%|██████████| 22/22 [01:12<00:00,  3.30s/it]\n",
      "100%|██████████| 23/23 [01:11<00:00,  3.10s/it]\n",
      "100%|██████████| 22/22 [01:09<00:00,  3.17s/it]\n",
      "100%|██████████| 15/15 [00:50<00:00,  3.34s/it]\n",
      "100%|██████████| 18/18 [00:58<00:00,  3.24s/it]\n",
      "100%|██████████| 19/19 [01:00<00:00,  3.21s/it]\n",
      "100%|██████████| 23/23 [01:16<00:00,  3.34s/it]\n",
      "100%|██████████| 21/21 [01:06<00:00,  3.15s/it]\n",
      "100%|██████████| 24/24 [01:17<00:00,  3.23s/it]\n",
      "100%|██████████| 21/21 [01:09<00:00,  3.31s/it]\n",
      "100%|██████████| 24/24 [01:14<00:00,  3.12s/it]\n",
      "100%|██████████| 19/19 [00:59<00:00,  3.15s/it]\n",
      "100%|██████████| 24/24 [01:15<00:00,  3.14s/it]\n",
      "100%|██████████| 17/17 [00:55<00:00,  3.25s/it]\n",
      "100%|██████████| 21/21 [01:07<00:00,  3.22s/it]\n",
      "100%|██████████| 20/20 [01:06<00:00,  3.31s/it]\n",
      "100%|██████████| 19/19 [01:03<00:00,  3.35s/it]\n",
      "100%|██████████| 20/20 [01:11<00:00,  3.57s/it]\n",
      "100%|██████████| 17/17 [00:57<00:00,  3.40s/it]\n",
      "100%|██████████| 24/24 [01:19<00:00,  3.29s/it]\n",
      "100%|██████████| 20/20 [01:02<00:00,  3.12s/it]\n",
      "100%|██████████| 24/24 [01:19<00:00,  3.30s/it]\n",
      "100%|██████████| 16/16 [00:53<00:00,  3.35s/it]\n",
      "100%|██████████| 19/19 [01:02<00:00,  3.28s/it]\n",
      "100%|██████████| 21/21 [01:06<00:00,  3.17s/it]\n",
      "100%|██████████| 20/20 [01:01<00:00,  3.05s/it]\n",
      "100%|██████████| 19/19 [01:03<00:00,  3.36s/it]\n",
      "100%|██████████| 18/18 [01:01<00:00,  3.43s/it]\n",
      "100%|██████████| 18/18 [01:00<00:00,  3.39s/it]\n",
      "100%|██████████| 22/22 [01:09<00:00,  3.17s/it]\n",
      "100%|██████████| 17/17 [00:56<00:00,  3.34s/it]\n",
      "100%|██████████| 21/21 [01:07<00:00,  3.21s/it]\n",
      "100%|██████████| 18/18 [01:00<00:00,  3.38s/it]\n",
      "100%|██████████| 21/21 [01:11<00:00,  3.41s/it]\n",
      "100%|██████████| 20/20 [01:04<00:00,  3.22s/it]\n",
      "100%|██████████| 19/19 [01:02<00:00,  3.31s/it]\n",
      "100%|██████████| 17/17 [01:00<00:00,  3.54s/it]\n",
      "100%|██████████| 26/26 [01:21<00:00,  3.12s/it]\n",
      "100%|██████████| 26/26 [01:21<00:00,  3.14s/it]\n",
      "100%|██████████| 17/17 [00:56<00:00,  3.31s/it]\n",
      "100%|██████████| 15/15 [00:51<00:00,  3.45s/it]\n",
      "100%|██████████| 14/14 [00:50<00:00,  3.62s/it]\n",
      "100%|██████████| 18/18 [01:01<00:00,  3.39s/it]\n",
      "100%|██████████| 15/15 [00:47<00:00,  3.14s/it]\n",
      "100%|██████████| 24/24 [01:16<00:00,  3.19s/it]\n",
      "100%|██████████| 22/22 [01:07<00:00,  3.08s/it]\n",
      "100%|██████████| 17/17 [00:59<00:00,  3.51s/it]\n",
      "100%|██████████| 18/18 [00:58<00:00,  3.23s/it]\n",
      "100%|██████████| 25/25 [01:18<00:00,  3.14s/it]\n",
      "100%|██████████| 22/22 [01:11<00:00,  3.24s/it]\n",
      "100%|██████████| 18/18 [00:58<00:00,  3.27s/it]\n",
      "100%|██████████| 17/17 [00:55<00:00,  3.26s/it]\n",
      "100%|██████████| 19/19 [01:01<00:00,  3.24s/it]\n",
      "100%|██████████| 16/16 [00:53<00:00,  3.37s/it]\n",
      "100%|██████████| 19/19 [01:06<00:00,  3.50s/it]\n",
      "100%|██████████| 21/21 [01:12<00:00,  3.46s/it]\n",
      "100%|██████████| 19/19 [01:03<00:00,  3.32s/it]\n",
      "100%|██████████| 18/18 [01:03<00:00,  3.51s/it]\n",
      "100%|██████████| 21/21 [01:10<00:00,  3.37s/it]\n",
      "100%|██████████| 22/22 [01:14<00:00,  3.37s/it]\n",
      "100%|██████████| 23/23 [01:12<00:00,  3.17s/it]\n",
      "100%|██████████| 18/18 [01:02<00:00,  3.50s/it]\n",
      "100%|██████████| 23/23 [01:13<00:00,  3.18s/it]\n",
      "100%|██████████| 24/24 [01:15<00:00,  3.14s/it]\n",
      "100%|██████████| 20/20 [01:03<00:00,  3.19s/it]\n",
      "100%|██████████| 21/21 [01:05<00:00,  3.13s/it]\n",
      "100%|██████████| 24/24 [01:22<00:00,  3.43s/it]\n",
      "100%|██████████| 22/22 [01:11<00:00,  3.24s/it]\n",
      "100%|██████████| 17/17 [00:54<00:00,  3.23s/it]\n",
      "100%|██████████| 18/18 [00:58<00:00,  3.22s/it]\n",
      "100%|██████████| 27/27 [01:30<00:00,  3.35s/it]\n",
      "100%|██████████| 24/24 [01:17<00:00,  3.22s/it]\n",
      "100%|██████████| 26/26 [01:27<00:00,  3.38s/it]\n",
      "100%|██████████| 25/25 [01:21<00:00,  3.27s/it]\n",
      "100%|██████████| 24/24 [01:13<00:00,  3.08s/it]\n",
      "100%|██████████| 25/25 [01:24<00:00,  3.38s/it]\n",
      "100%|██████████| 23/23 [01:12<00:00,  3.15s/it]\n",
      "100%|██████████| 23/23 [01:13<00:00,  3.22s/it]\n",
      "100%|██████████| 28/28 [01:31<00:00,  3.28s/it]\n",
      "100%|██████████| 23/23 [01:14<00:00,  3.23s/it]\n",
      "100%|██████████| 25/25 [01:19<00:00,  3.17s/it]\n",
      "100%|██████████| 25/25 [01:18<00:00,  3.14s/it]\n",
      "100%|██████████| 21/21 [01:18<00:00,  3.72s/it]\n",
      "100%|██████████| 21/21 [01:10<00:00,  3.35s/it]\n",
      "100%|██████████| 25/25 [01:20<00:00,  3.24s/it]\n",
      "100%|██████████| 18/18 [01:04<00:00,  3.60s/it]\n",
      "100%|██████████| 21/21 [01:08<00:00,  3.27s/it]\n",
      "100%|██████████| 20/20 [01:06<00:00,  3.34s/it]\n",
      "100%|██████████| 20/20 [01:09<00:00,  3.47s/it]\n",
      "100%|██████████| 23/23 [01:21<00:00,  3.55s/it]\n",
      "100%|██████████| 27/27 [01:28<00:00,  3.26s/it]\n",
      "100%|██████████| 23/23 [01:13<00:00,  3.20s/it]\n",
      "100%|██████████| 17/17 [00:55<00:00,  3.29s/it]\n",
      "100%|██████████| 22/22 [01:15<00:00,  3.41s/it]\n",
      "100%|██████████| 22/22 [01:12<00:00,  3.30s/it]\n",
      "100%|██████████| 23/23 [01:14<00:00,  3.26s/it]\n",
      "100%|██████████| 24/24 [01:17<00:00,  3.22s/it]\n",
      "100%|██████████| 20/20 [01:11<00:00,  3.57s/it]\n",
      "100%|██████████| 19/19 [01:05<00:00,  3.42s/it]\n",
      "100%|██████████| 19/19 [01:02<00:00,  3.31s/it]\n",
      "100%|██████████| 23/23 [01:14<00:00,  3.26s/it]\n",
      "100%|██████████| 22/22 [01:11<00:00,  3.26s/it]\n",
      "100%|██████████| 19/19 [01:02<00:00,  3.30s/it]\n",
      "100%|██████████| 25/25 [01:19<00:00,  3.20s/it]\n",
      "100%|██████████| 23/23 [01:20<00:00,  3.48s/it]\n",
      "100%|██████████| 23/23 [01:16<00:00,  3.34s/it]\n",
      "100%|██████████| 20/20 [01:06<00:00,  3.32s/it]\n",
      "100%|██████████| 22/22 [01:18<00:00,  3.55s/it]\n",
      "100%|██████████| 22/22 [01:17<00:00,  3.50s/it]\n",
      "100%|██████████| 25/25 [01:25<00:00,  3.42s/it]\n",
      "100%|██████████| 18/18 [01:03<00:00,  3.52s/it]\n",
      "100%|██████████| 23/23 [01:14<00:00,  3.22s/it]\n",
      "100%|██████████| 21/21 [01:11<00:00,  3.41s/it]\n",
      "100%|██████████| 19/19 [01:03<00:00,  3.35s/it]\n",
      "100%|██████████| 20/20 [01:09<00:00,  3.47s/it]\n",
      "100%|██████████| 20/20 [01:13<00:00,  3.67s/it]\n",
      "100%|██████████| 21/21 [01:08<00:00,  3.28s/it]\n",
      "100%|██████████| 17/17 [01:00<00:00,  3.55s/it]\n",
      "100%|██████████| 27/27 [01:34<00:00,  3.51s/it]\n",
      "100%|██████████| 23/23 [01:19<00:00,  3.47s/it]\n",
      "100%|██████████| 25/25 [01:24<00:00,  3.37s/it]\n",
      "100%|██████████| 25/25 [01:22<00:00,  3.31s/it]\n",
      "100%|██████████| 25/25 [01:25<00:00,  3.41s/it]\n",
      "100%|██████████| 29/29 [01:30<00:00,  3.13s/it]\n",
      "100%|██████████| 22/22 [01:20<00:00,  3.64s/it]\n",
      "100%|██████████| 19/19 [01:10<00:00,  3.73s/it]\n",
      "100%|██████████| 15/15 [01:00<00:00,  4.01s/it]\n",
      "100%|██████████| 25/25 [01:27<00:00,  3.52s/it]\n",
      "100%|██████████| 24/24 [01:24<00:00,  3.50s/it]\n",
      "100%|██████████| 20/20 [01:07<00:00,  3.36s/it]\n",
      "100%|██████████| 26/26 [01:30<00:00,  3.48s/it]\n",
      "100%|██████████| 26/26 [01:30<00:00,  3.49s/it]\n",
      "100%|██████████| 23/23 [01:19<00:00,  3.47s/it]\n",
      "100%|██████████| 24/24 [01:26<00:00,  3.60s/it]\n",
      "100%|██████████| 21/21 [01:17<00:00,  3.67s/it]\n",
      "100%|██████████| 22/22 [01:25<00:00,  3.88s/it]\n",
      "100%|██████████| 24/24 [01:34<00:00,  3.96s/it]\n",
      "100%|██████████| 19/19 [01:15<00:00,  3.97s/it]\n",
      "100%|██████████| 23/23 [01:24<00:00,  3.67s/it]\n",
      "100%|██████████| 16/16 [00:58<00:00,  3.67s/it]\n",
      "100%|██████████| 24/24 [01:32<00:00,  3.85s/it]\n",
      "100%|██████████| 27/27 [01:34<00:00,  3.49s/it]\n",
      "100%|██████████| 29/29 [01:45<00:00,  3.62s/it]\n",
      "100%|██████████| 27/27 [01:41<00:00,  3.76s/it]\n",
      "100%|██████████| 24/24 [01:32<00:00,  3.84s/it]\n",
      "100%|██████████| 27/27 [01:35<00:00,  3.55s/it]\n",
      "100%|██████████| 25/25 [01:35<00:00,  3.80s/it]\n",
      "100%|██████████| 25/25 [01:40<00:00,  4.01s/it]\n",
      "100%|██████████| 24/24 [01:38<00:00,  4.11s/it]\n",
      "100%|██████████| 15/15 [00:59<00:00,  3.98s/it]\n",
      "100%|██████████| 13/13 [00:49<00:00,  3.80s/it]\n"
     ]
    }
   ],
   "source": [
    "df_outputs = []\n",
    "not_worked = []\n",
    "start_time = time.time()\n",
    "\n",
    "for batch in batches:\n",
    "    for i, row in tqdm(batch.iterrows(), total=len(batch)):\n",
    "        try:\n",
    "            article_text = f'Title: {row['Title']}, Content: {row['Content']}'\n",
    "            outputs = overall_chain.invoke({\n",
    "                'text': article_text,\n",
    "                'format_instructions': format_instructions\n",
    "            })\n",
    "            output_dict = {'GOID': row['GOID'], 'company': outputs['company']}\n",
    "            output_dict.update(output_parser.parse(outputs['json_output']))\n",
    "            df_outputs.append(output_dict)\n",
    "        except:\n",
    "            not_worked.append(row['GOID'])\n",
    "            \n",
    "        if i % 2 == 0:\n",
    "            pd.DataFrame.from_dict(df_outputs).to_csv('../data/final/llm_outputs.csv', index=False)\n",
    "            np.array(not_worked).tofile('../data/final/did_not_work.txt', sep=',')\n",
    "\n",
    "    time_elapsed = time.time() - start_time\n",
    "    if time_elapsed < REQUEST_INTERVAL:\n",
    "        time.sleep(REQUEST_INTERVAL - time_elapsed)\n",
    "        start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Hackathon_2024-ADjn7pna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "23799e209aef547641389b0291214a94bccee1058c0bbacbaa006711bc56d570"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
